{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Text RNN Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from simple_model import Model\n",
    "import codecs\n",
    "import collections\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../author_data'# data directory containing input.txt\n",
    "input_encoding = None # character encoding of input.txt, from https://docs.python.org/3/library/codecs.html#standard-encodings'\n",
    "log_dir = 'logs'# directory containing tensorboard logs\n",
    "save_dir = 'save' # directory to store checkpointed models\n",
    "rnn_size = 256 # size of RNN hidden state\n",
    "num_layers = 2 # number of layers in the RNN\n",
    "model = 'lstm' # lstm model\n",
    "batch_size = 50 # minibatch size\n",
    "seq_length = 25 # RNN sequence length\n",
    "num_epochs = 2 # number of epochs\n",
    "save_every = 1000 # save frequency\n",
    "grad_clip = 5. #clip gradients at this value\n",
    "learning_rate= 0.002 #learning rate\n",
    "decay_rate = 0.97 #decay rate for rmsprop\n",
    "gpu_mem = 0.666 #%% of gpu memory to be allocated to this process. Default is 66.6%%\n",
    "init_from = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = os.path.join(data_dir, \"eap_train.txt\")\n",
    "vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "with codecs.open(input_file, \"r\", encoding=None) as f:\n",
    "    data = f.read()\n",
    "x_text = data.split() #Split Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count the number of words\n",
    "word_counts = collections.Counter(x_text)\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "words = [x[0] for x in word_counts.most_common()]\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(vocab_file, 'wb') as f:\n",
    "    cPickle.dump((words), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor is:[ 3739 19493 13829 ..., 25438  8363 20219]\n",
      "It's shape: (200995,)\n"
     ]
    }
   ],
   "source": [
    "tensor = np.array(list(map(vocab.get, x_text)))\n",
    "\n",
    "# Save the data to data.npy\n",
    "np.save(tensor_file, tensor)\n",
    "\n",
    "print('tensor is:' + str(tensor))\n",
    "print(\"It's shape: \" + str(np.shape(tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches is: 160\n",
      "The shape of the new tensor is: (200000,)\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(tensor.size / (batch_size * seq_length))\n",
    "print('Number of batches is: ' + str(num_batches))\n",
    "tensor = tensor[:num_batches * batch_size * seq_length]\n",
    "print('The shape of the new tensor is: '+ str(np.shape(tensor)))\n",
    "\n",
    "xdata = tensor\n",
    "ydata = np.copy(tensor)\n",
    "ydata[:-1] = xdata[1:]\n",
    "ydata[-1] = xdata[0]\n",
    "x_batches = np.split(xdata.reshape(batch_size, -1), num_batches, 1)\n",
    "y_batches = np.split(ydata.reshape(batch_size, -1), num_batches, 1)\n",
    "\n",
    "pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(save_dir, 'words_vocab.pkl'), 'wb') as f:\n",
    "    cPickle.dump((words, vocab), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Model\n",
    "(utilizes simple_model.py as a model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(data_dir,input_encoding,log_dir,save_dir,rnn_size,num_layers,model,batch_size,seq_length,num_epochs,save_every,grad_clip,learning_rate,decay_rate,gpu_mem,init_from, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/320 (epoch 0), train_loss = 10.184, time/batch = 4.505\n",
      "model saved to save/model_test.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        #add the session graph to the writer\n",
    "        train_writer.add_graph(sess.graph)\n",
    "\n",
    "        #initialize global variables\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        #create the Saver to save the model and its variables.\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        #create a for loop, to run over all epochs (defined as e)\n",
    "        for e in range(model.epoch_pointer.eval(), num_epochs):\n",
    "            #a session encapsulates the environement in which operations objects are executed.\n",
    "                        \n",
    "            #Initialization:\n",
    "            \n",
    "            #here we assign to the lr (learning rate) value of the model, the value : args.learning_rate * (args.decay_rate ** e))\n",
    "            sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "            \n",
    "            #we define the state of the model. At the beginning, its the initial state of the model.\n",
    "            state = sess.run(model.initial_state)\n",
    "            #speed to 0 at the beginning.\n",
    "            speed = 0\n",
    "            #reinitialize pointer for batches\n",
    "            pointer = 0\n",
    "            \n",
    "            if init_from is None:\n",
    "                assign_op = model.epoch_pointer.assign(e)\n",
    "                sess.run(assign_op)\n",
    "\n",
    "            if init_from is not None:\n",
    "                pointer = model.batch_pointer.eval()\n",
    "                init_from = None\n",
    "\n",
    "            #in each epoch, for loop to run over each batch (b)\n",
    "            for b in range(pointer, num_batches):\n",
    "                #define the starting date:\n",
    "                start = time.time()\n",
    "                #define x and y for the next batch\n",
    "                x, y = x_batches[pointer], y_batches[pointer]\n",
    "                pointer += 1\n",
    "\n",
    "                #create the feeding string for the model.\n",
    "                #input data are x, targets are y, the initiate state is state, and batch time 0.\n",
    "                feed = {model.input_data: x, model.targets: y, model.initial_state: state,\n",
    "                        model.batch_time: speed}\n",
    "\n",
    "                #run the session and train.\n",
    "                summary, train_loss, state, _, _ = sess.run([merged, model.cost, model.final_state,\n",
    "                                                             model.train_op, model.inc_batch_pointer_op], feed)\n",
    "                #add summary to the log\n",
    "                train_writer.add_summary(summary, e * num_batches + b)\n",
    "\n",
    "                #calculate the speed of the batch.\n",
    "                #this information will be displayed later.\n",
    "                speed = time.time() - start\n",
    "\n",
    "                #display something in the console\n",
    "                #---------------------------------\n",
    "                #print information:\n",
    "                if (e * num_batches + b) % batch_size == 0:\n",
    "                    print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                        .format(e * num_batches + b,\n",
    "                                num_epochs * num_batches,\n",
    "                                e, train_loss, speed))\n",
    "                \n",
    "                #save model:\n",
    "                if (e * num_batches + b) % save_every == 0 \\\n",
    "                        or (e==num_epochs-1 and b == num_batches-1): # save for the last result\n",
    "                    #define the path to the model\n",
    "                    checkpoint_path = os.path.join(save_dir, 'model_test.ckpt')\n",
    "                    #save the model, woth increment ()\n",
    "                    saver.save(sess, checkpoint_path, global_step = e * num_batches + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n",
    "        \n",
    "        #close the session\n",
    "        train_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
